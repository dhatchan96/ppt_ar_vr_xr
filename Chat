from concurrent.futures import ThreadPoolExecutor, as_completed

def upload(self, results, externalRunID, applicationModuleID, testSuiteID):
    sliceSize = self.octaneConfig.get("uploadedSliceSize")
    slice = 0
    max_retries = 5
    running_tasks = []
    num_threads = 5   # Number of parallel uploads

    logger.info(f"Starting to Upload {len(results)} automated runs to Octane in batches with {sliceSize} runs per task.")

    slices = [results[i:i+sliceSize] for i in range(0, len(results), sliceSize)]  # Pre-split all slices

    def upload_single_slice(resultsSlice):
        testRuns = [
            TEST_RUN.format(
                testname=testname.split('.')[-1],
                testclass=testname.split('.')[-2],
                status=status,
                started=started,
                duration=duration,
                package=testname.split('.')[-3],
                module=testname.split('.')[-4]
            )
            for (testname, testclass, status, started, duration) in resultsSlice
        ]

        data = EVIDENCE_TEMPLATE_TEST_SUITE.format(
            test_results='\n'.join(testRuns),
            testSuiteID=testSuiteID,
            externalRunID=externalRunID,
            applicationModuleID=applicationModuleID
        )

        response = self.endpoint.post("test-results", data=data, dataType="xml")
        status = response
        task_response = response
        task_id = task_response.get("id")

        if status == "Success":
            logger.info(f"Upload successful for a batch. Task ID: {task_id}")
        else:
            logger.error(f"Upload failed for a batch: {response}")
            raise RuntimeError(f"Failed to upload test results slice: {response}")

        # Retry task status check
        retry_count = 0
        while retry_count < max_retries:
            task_status_resp = self.endpoint.get(f"test-results/{task_id}")
            task_status = task_status_resp.get("status")

            if task_status.upper() == "SUCCESS":
                logger.info(f"Upload task {task_id} completed successfully.")
                break
            elif task_status.upper() in ("FAILED", "ERROR"):
                logger.error(f"Upload task {task_id} failed: {task_status_resp.get('errorDetails')}")
                raise RuntimeError(f"Upload task {task_id} failed.")
            elif task_status.upper() in ("RUNNING", "QUEUED"):
                if retry_count == max_retries - 1:
                    logger.warning(f"Task {task_id} still {task_status.upper()} after {max_retries} retries. Storing for later tracking.")
                    running_tasks.append(task_id)
                    break
                else:
                    logger.info(f"Task {task_id} still {task_status.upper()}. Retrying ({retry_count + 1}/{max_retries})...")
                    retry_count += 1
                    time.sleep(5)
            else:
                logger.warning(f"Unexpected task status {task_status.upper()} for task {task_id}. Moving on.")
                break

        return task_id

    # Launch multiple threads to upload slices in parallel
    with ThreadPoolExecutor(max_workers=num_threads) as executor:
        futures = [executor.submit(upload_single_slice, resultsSlice) for resultsSlice in slices]

        for future in as_completed(futures):
            try:
                task_id = future.result()
                logger.info(f"Upload completed for task_id: {task_id}")
            except Exception as e:
                logger.error(f"Upload failed for a slice: {e}")

    return running_tasks
